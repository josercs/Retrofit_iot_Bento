[agent]
  interval = "5s"
  round_interval = true
  flush_interval = "5s"
  debug = true

[[inputs.mqtt_consumer]]
  servers = ["ssl://mosquitto:8883"]
  username = "telegraf"
  password = "$TELEGRAF_MQTT_PASSWORD"
  # Permite roteamento multi-tenant por tópico, ex.: plc/<tenant>/<plc>
  topics = ["plc/#", "plc/db1"]
  # Adiciona o tópico como tag para parsers posteriores
  topic_tag = "topic"
  data_format = "json_v2"
  tls_ca = "/etc/telegraf/certs/ca.crt"
  insecure_skip_verify = false
  # Use sessão limpa para garantir que as inscrições reflitam este arquivo
  # (evita ficar preso em inscrições antigas mantidas pelo broker)
  persistent_session = false
  client_id = "telegraf_mirror"
  qos = 1
  [[inputs.mqtt_consumer.json_v2]]
  measurement_name = "s7_db1"
    # promote top-level ip and db into tags for easy filtering in Flux/Grafana
    [[inputs.mqtt_consumer.json_v2.tag]]
      path = "ip"
      rename = "ip"
    [[inputs.mqtt_consumer.json_v2.tag]]
      path = "db"
      rename = "db"
      # tenant_id e plc_id no JSON são opcionais; se vierem, capture como fields
      [[inputs.mqtt_consumer.json_v2.field]]
        path = "tenant_id"
        rename = "tenant_id"
        optional = true
      [[inputs.mqtt_consumer.json_v2.field]]
        path = "plc_id"
        rename = "plc_id"
        optional = true
    # Ingerir automaticamente todos os filhos de values (numéricos/booleanos)
    [[inputs.mqtt_consumer.json_v2.object]]
      path = "values"

[[inputs.prometheus]]
  urls = ["http://edge-collector:9108/metrics"]
  response_timeout = "10s"
  metric_version = 2

[[outputs.influxdb_v2]]
  urls = ["http://influxdb:8086"]
  token = "$INFLUX_TOKEN"
  organization = "planta"
  bucket = "processo"
  timeout = "10s"
  content_encoding = "gzip"
  # Ajuste para reduzir overhead e evitar perdas por overflow
  metric_batch_size = 100
  metric_buffer_limit = 1000

[[outputs.influxdb_v2]]
  # Bucket bruto sem retenção para downsampling
  urls = ["http://influxdb:8086"]
  token = "$INFLUX_TOKEN"
  organization = "planta"
  bucket = "processo_raw"
  timeout = "10s"
  content_encoding = "gzip"
  metric_batch_size = 100
  metric_buffer_limit = 1000

[[processors.rename]]
  [[processors.rename.replace]]
    field = "plc_id"
    dest = "plc_id"
  # Se plc_id não vier, usar ip como fallback em outro processor (exemplo simplificado)

[[processors.converter]]
  [processors.converter.tags]
  string = ["plc_id", "tenant_id"]
  [processors.converter.fields]
    integer = []

# Derivação condicional a partir do tópico (só se a tag NÃO existir)
# 1) Preencher tenant_id a partir de plc/<tenant>/<plc> quando tenant_id estiver ausente
[[processors.regex]]
  namepass = ["s7_db1"]
  # Não processar este bloco se já existir tenant_id (qualquer valor)
  tagdrop = { tenant_id = ["*"] }
  [[processors.regex.tags]]
    key = "topic"
    pattern = "^plc/([^/]+)/([^/]+)"
    replacement = "${1}"
    result_key = "tenant_id"

# 2) Preencher plc_id a partir de plc/<tenant>/<plc> quando plc_id estiver ausente
[[processors.regex]]
  namepass = ["s7_db1"]
  # Não processar este bloco se já existir plc_id (qualquer valor)
  tagdrop = { plc_id = ["*"] }
  [[processors.regex.tags]]
    key = "topic"
    pattern = "^plc/([^/]+)/([^/]+)"
    replacement = "${2}"
    result_key = "plc_id"

# 3) Fallback final: se ainda não houver plc_id, definir para HOSTNAME (não sobrescreve existentes)
[[processors.override]]
  namepass = ["s7_db1"]
  # Pula este processor se já existir plc_id
  tagdrop = { plc_id = ["*"] }
  [processors.override.tags]
    plc_id = "${HOSTNAME}"  # fallback

[[outputs.file]]
  files = ["/tmp/telegraf_out.lp"]
  data_format = "influx"
